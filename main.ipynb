{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo import AECEnv\n",
    "from utils import write_to_file\n",
    "from pathlib import Path\n",
    "from agent_factory import AgentFactory, DirectPromptAgentFactory, ReflectionAgentFactory, DummyAgentFactory\n",
    "from configs import *\n",
    "from agent import IAgent\n",
    "import concurrent.futures\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import rps, prisoners_dilemma\n",
    "\n",
    "env_config = prisoners_dilemma.env_config\n",
    "\n",
    "lm_config = LMConfig(\n",
    "    gpt_model = 'gpt-3.5-turbo',\n",
    "    max_tokens=1400,\n",
    "    log_path=Path('./log/'),\n",
    "    log_file=Path('lm_log.txt'),\n",
    ")\n",
    "\n",
    "def direct_agent_factory() -> AgentFactory:\n",
    "    return DirectPromptAgentFactory()\n",
    "\n",
    "def reflection_agent_factory() -> AgentFactory:\n",
    "    return ReflectionAgentFactory()\n",
    "\n",
    "get_agent_factories: Dict[str, Callable[..., AgentFactory]] = {\n",
    "    \"direct\": direct_agent_factory,\n",
    "    \"reflection\": reflection_agent_factory,\n",
    "}\n",
    "\n",
    "def simulate(agents: Dict[str, IAgent], env: AECEnv, agent_name: str) -> dict[any, float]:\n",
    "    env.reset()\n",
    "    game_history = f\"Your agent is named {agent_name}.\\n\"\n",
    "    rewards = {agent: 0 for agent in env.possible_agents}\n",
    "    \n",
    "    for agent in agents.values():\n",
    "        agent.reset()\n",
    "\n",
    "    for agent_key in env.agent_iter():\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "        rewards[agent_key] += reward\n",
    "        agents[agent_key].observe(\n",
    "            observation, reward, termination, truncation, info\n",
    "        )\n",
    "        if termination or truncation:\n",
    "            action = None\n",
    "        else:\n",
    "            try:\n",
    "                action = agents[agent_key].act()\n",
    "            except Exception as e:\n",
    "                raise(f\"Error in {agent_key}: {e}\")\n",
    "        game_history += f\"{agent_key} takes action {action}\\n\"\n",
    "        env.step(action)\n",
    "    env.close()\n",
    "    return rewards, game_history\n",
    "\n",
    "@dataclass\n",
    "class Result:\n",
    "    agent_factory: str\n",
    "    baseline: str\n",
    "    go_first: int\n",
    "    id_trial: int\n",
    "    id_iter: int\n",
    "    define_agent_code: str\n",
    "    rewards: dict[str, float]\n",
    "    game_history: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(env_config.prompt_get_initial_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "        agent_factory_name: str,\n",
    "        get_agent_factory: Callable[..., AgentFactory],\n",
    "        baseline_name: str,\n",
    "        baseline_class: type,\n",
    "        go_first: int,\n",
    "        id_trial: int,\n",
    "        env_config: EnvConfig,\n",
    "        lm_config: LMConfig\n",
    "    ):\n",
    "    experiment_results = []\n",
    "    agent_factory = get_agent_factory()\n",
    "    for id_iter in range(2):\n",
    "        Agent, define_agent_code = agent_factory.produce_agent_class(env_config, lm_config)\n",
    "        env = env_config.get_environment()\n",
    "        agents = {}\n",
    "        for i, name in enumerate(env.possible_agents):\n",
    "            if i == go_first:\n",
    "                agent_name = name\n",
    "                agents[name] = Agent(env, name)\n",
    "            else:\n",
    "                agents[name] = baseline_class(env, name)\n",
    "        rewards, game_history = simulate(agents, env, agent_name)\n",
    "        agent_factory.update(game_history)\n",
    "        experiment_results.append(\n",
    "            {\n",
    "                \"agent_factory\": agent_factory_name,\n",
    "                \"baseline\": baseline_name,\n",
    "                \"go_first\": go_first,\n",
    "                \"id_trial\": id_trial,\n",
    "                \"id_iter\": id_iter,\n",
    "                \"define_agent_code\": define_agent_code,\n",
    "                \"rewards\": rewards,\n",
    "                \"game_history\": game_history\n",
    "            }\n",
    "        )\n",
    "    return experiment_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Agent:\n",
    "    def __init__(self, env, name):\n",
    "        self.env = env\n",
    "        self.name = name\n",
    "        self.actions_history = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.actions_history = []\n",
    "\n",
    "    def observe(self, observation, reward, termination, truncation, info):\n",
    "        if observation is not None:\n",
    "            self.actions_history.append(observation)\n",
    "\n",
    "    def act(self):\n",
    "        if len(self.actions_history) == 0:\n",
    "            return np.random.choice([0, 1])  # Random choice for the first round\n",
    "\n",
    "        cooperate_count = np.sum(np.array(self.actions_history) == 0)\n",
    "        betray_count = np.sum(np.array(self.actions_history) == 1)\n",
    "\n",
    "        return 0 if cooperate_count >= betray_count else 1  # Cooperate if ties\n",
    "    \n",
    "dummy_factory = lambda: DummyAgentFactory(Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'agent_factory': 'reflect',\n",
       "  'baseline': 'coop',\n",
       "  'go_first': 0,\n",
       "  'id_trial': 0,\n",
       "  'id_iter': 0,\n",
       "  'define_agent_code': '\\nclass Agent:\\n    import random\\n\\n    def __init__(self, env, name):\\n        self.env = env\\n        self.name = name\\n        self.history = {\\n            \"observations\": [],\\n            \"actions\": []\\n        }\\n\\n    def reset(self):\\n        self.history = {\\n            \"observations\": [],\\n            \"actions\": []\\n        }\\n\\n    def observe(self, observation, reward, termination, truncation, info):\\n        self.history[\"observations\"].append(observation)\\n\\n    def act(self):\\n        if len(self.history[\"actions\"]) == 0:\\n            action = self.random_action()\\n        else:\\n            opponent_action = self.history[\"observations\"][-1]\\n            action = self.pavlov_strategy(opponent_action)\\n        self.history[\"actions\"].append(action)\\n        return action\\n\\n    def random_action(self):\\n        return self.env.action_space(self.name).sample()\\n\\n    def pavlov_strategy(self, opponent_action):\\n        if opponent_action == 0:\\n            return self.history[\"actions\"][-1]\\n        else:\\n            return 1 - self.history[\"actions\"][-1]\\n',\n",
       "  'rewards': {'player_0': -15, 'player_1': -15},\n",
       "  'game_history': 'Your agent is named player_0.\\nplayer_0 takes action 0\\nplayer_1 takes action 0\\nplayer_0 takes action 0\\nplayer_1 takes action 0\\nplayer_0 takes action 0\\nplayer_1 takes action 0\\nplayer_0 takes action 0\\nplayer_1 takes action 0\\nplayer_0 takes action 0\\nplayer_1 takes action 0\\nplayer_0 takes action 0\\nplayer_1 takes action 0\\nplayer_0 takes action 0\\nplayer_1 takes action 0\\nplayer_0 takes action 0\\nplayer_1 takes action 0\\nplayer_0 takes action 0\\nplayer_1 takes action 0\\nplayer_0 takes action 0\\nplayer_1 takes action 0\\nplayer_0 takes action 0\\nplayer_1 takes action 0\\nplayer_0 takes action 0\\nplayer_1 takes action 0\\nplayer_0 takes action 0\\nplayer_1 takes action 0\\nplayer_0 takes action 0\\nplayer_1 takes action 0\\nplayer_0 takes action 0\\nplayer_1 takes action 0\\nplayer_0 takes action None\\nplayer_1 takes action None\\n'},\n",
       " {'agent_factory': 'reflect',\n",
       "  'baseline': 'coop',\n",
       "  'go_first': 0,\n",
       "  'id_trial': 0,\n",
       "  'id_iter': 1,\n",
       "  'define_agent_code': '\\nclass Agent:\\n    def __init__(self, env, name):\\n        self.env = env\\n        self.name = name\\n        self.history = {\\n            \"observations\": [],\\n            \"actions\": []\\n        }\\n\\n    def reset(self):\\n        self.history = {\\n            \"observations\": [],\\n            \"actions\": []\\n        }\\n\\n    def observe(self, observation, reward, termination, truncation, info):\\n        self.history[\"observations\"].append(observation)\\n\\n    def act(self):\\n        action = 1  # Always choose to betray\\n        self.history[\"actions\"].append(action)\\n        return action\\n',\n",
       "  'rewards': {'player_0': 0, 'player_1': -45},\n",
       "  'game_history': 'Your agent is named player_0.\\nplayer_0 takes action 1\\nplayer_1 takes action 0\\nplayer_0 takes action 1\\nplayer_1 takes action 0\\nplayer_0 takes action 1\\nplayer_1 takes action 0\\nplayer_0 takes action 1\\nplayer_1 takes action 0\\nplayer_0 takes action 1\\nplayer_1 takes action 0\\nplayer_0 takes action 1\\nplayer_1 takes action 0\\nplayer_0 takes action 1\\nplayer_1 takes action 0\\nplayer_0 takes action 1\\nplayer_1 takes action 0\\nplayer_0 takes action 1\\nplayer_1 takes action 0\\nplayer_0 takes action 1\\nplayer_1 takes action 0\\nplayer_0 takes action 1\\nplayer_1 takes action 0\\nplayer_0 takes action 1\\nplayer_1 takes action 0\\nplayer_0 takes action 1\\nplayer_1 takes action 0\\nplayer_0 takes action 1\\nplayer_1 takes action 0\\nplayer_0 takes action 1\\nplayer_1 takes action 0\\nplayer_0 takes action None\\nplayer_1 takes action None\\n'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_experiment(\"reflect\", reflection_agent_factory, \"coop\", env_config.baselines[\"coop\"], 0, 0, env_config, lm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This will collect all results from the experiments\n",
    "all_experiment_results = []\n",
    "\n",
    "# Using ProcessPoolExecutor for parallel execution\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Prepare list of futures\n",
    "    futures = []\n",
    "    for agent_factory_name, get_agent_factory in get_agent_factories.items():\n",
    "        for baseline_name, baseline_class in env_config.baselines.items():\n",
    "            for go_first in range(2):\n",
    "                for id_trial in range(3):\n",
    "                    # Schedule the execution of each experiment\n",
    "                    future = executor.submit(run_experiment, agent_factory_name, get_agent_factory, baseline_name, baseline_class, go_first, id_trial, env_config, lm_config)\n",
    "                    futures.append(future)\n",
    "\n",
    "    # Collect results as they are completed\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        all_experiment_results.extend(future.result())\n",
    "\n",
    "# all_experiment_results now contains results from all experiments"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
