Prompt: 
You are given a task to write a game agent to interact with a pettingzoo environment.

Here is the some information regarding the environment:
# Rock Paper Scissors
| Import             | `from pettingzoo.classic import rps_v2` |
|--------------------|-----------------------------------------|
| Actions            | Discrete                                |
| Parallel API       | Yes                                     |
| Manual Control     | No                                      |
| Agents             | `agents= ['player_0', 'player_1']`      |
| Agents             | 2                                       |
| Action Shape       | Discrete(3)                             |
| Action Values      | Discrete(3)                             |
| Observation Shape  | Discrete(4)                             |
| Observation Values | Discrete(4)                             |


Rock, Paper, Scissors is a 2-player hand game where each player chooses either rock, paper or scissors and reveals their choices simultaneously. If both players make the same choice, then it is a draw. However, if their choices are different, the winner is determined as follows: rock beats
scissors, scissors beat paper, and paper beats rock.

### Arguments

``` python
rps_v2.env(max_cycles=15)
```
`max_cycles`:  after max_cycles steps all agents will return done.

### Observation Space

#### Rock, Paper, Scissors

If 3 actions are required, the game played is the standard Rock, Paper, Scissors. The observation is the last opponent action and its space is a scalar value with 4 possible values. Since both players reveal their choices at the same time, the observation is None until both players have acted.
Therefore, 3 represents no action taken yet. Rock is represented with 0, paper with 1 and scissors with 2.

| Value  |  Observation |
| :----: | :---------:  |
| 0      | Rock         |
| 1      | Paper        |
| 2      | Scissors     |
| 3      | None         |


### Action Space

#### Rock, Paper, Scissors

The action space is a scalar value with 3 possible values. The values are encoded as follows: Rock is 0, paper is 1 and scissors is 2.

| Value  |  Action |
| :----: | :---------:  |
| 0      | Rock         |
| 1      | Paper        |
| 2      | Scissors     |

### Rewards

| Winner | Loser |
| :----: | :---: |
| +1     | -1    |

If the game ends in a draw, both players will receive a reward of 0.

The code describe how your agent will interact with the environment:
```python
from pettingzoo.classic import game_name_v0

# create the environment
env = game_name_v0.env()

# create your agent and the opponent agent
agents = {
    env.possible_agents[0]: Agent(env, env.possible_agents[0]),
    env.possible_agents[1]: OpponentAgent(env, env.possible_agents[1])
}

def simulate(agents: any, env: AECEnv) -> dict[any, float]:
    env.reset()
    rewards = {agent: 0 for agent in env.possible_agents}
    
    for agent in agents.values():
        agent.reset()

    for agent_name in env.agent_iter():
        observation, reward, termination, truncation, info = env.last()
        rewards[agent_name] += reward
        obs_message = agents[agent_name].observe(
            observation, reward, termination, truncation, info
        )
        if termination or truncation:
            action = None
        else:
            action = agents[agent_name].act()
        print(f"Agen {agent_name} Action {action}")
        env.step(action)
    env.close()
    return rewards

simulate(agents, env)
```

Here is a starter code to help you get started with coding the agent class,
you can replace the starter code with your own code and add more methods if needed,
but you should not change the method signatures:
# This is the starting point of your agent class, you should not change the existing method signatures.
# However, you can add more methods or change the implementations to obtain a better strategy.
class Agent:
    # import the necessary packages here
    import random
    # initialize the agent's state here
    def __init__(self, env, name):
        self.env = env
        self.name = name
        self.history = {
            "observations": [],
            "actions": []
        }

    # reset the agent's state here
    def reset(self):
        """
        Reset the agent's state.
        """
        self.history = {
            "observations": [],
            "actions": []
        }
    
    # update the agent's state here
    # This is an example of storing the observation.
    def observe(self, observation, reward, termination, truncation, info):
        self.history["observations"].append(observation)
    
    # return the action here
    # write comments to explain why this is a good policy.
    # this is an example of choosing the action randomly, you may want to change it.
    # self.env.action_space is a method, not a subscriptable object.
    def act(self):
        action = self.env.action_space(self.name).sample()
        self.history["actions"].append(action)
        return action

Now, you are ready to response to the challenge with the following steps.
- think step by step, recap some basic game theory knowledge and apply them to the problem.
- given the starter code, you should write a new Agent class to interact with the environment and maximize your reward. 
For the code part, you should be aware of the following:
- enclose the code by ```python and ```
- only write the Agent class
- all relevant information including `import` need to be under the Agent scope.
- for each method, you should write comments to describe the method.
Response: 
```python
# This is the starting point of your agent class, you should not change the existing method signatures.
# However, you can add more methods or change the implementations to obtain a better strategy.
class Agent:
    # import the necessary packages here
    import random
    
    # initialize the agent's state here
    def __init__(self, env, name):
        self.env = env
        self.name = name
        self.history = {
            "observations": [],
            "actions": []
        }

    # reset the agent's state here
    def reset(self):
        """
        Reset the agent's state.
        """
        self.history = {
            "observations": [],
            "actions": []
        }
    
    # update the agent's state here
    # This is an example of storing the observation.
    def observe(self, observation, reward, termination, truncation, info):
        self.history["observations"].append(observation)
    
    # return the action here
    # write comments to explain why this is a good policy.
    # this is an example of choosing the action randomly, you may want to change it.
    # self.env.action_space is a method, not a subscriptable object.
    def act(self):
        action = self.env.action_space(self.name).sample()
        self.history["actions"].append(action)
        return action
```

To maximize the reward in the Rock Paper Scissors game, we can implement a simple strategy based on the opponent's previous actions. Here's an updated version of the Agent class:

```python
class Agent:
    import random
    
    def __init__(self, env, name):
        self.env = env
        self.name = name
        self.history = {
            "observations": [],
            "actions": []
        }
        self.opponent_previous_action = None
    
    def reset(self):
        self.history = {
            "observations": [],
            "actions": []
        }
        self.opponent_previous_action = None
    
    def observe(self, observation, reward, termination, truncation, info):
        self.history["observations"].append(observation)
        if self.name != observation[0]:
            self.opponent_previous_action = observation[1]
    
    def act(self):
        if self.opponent_previous_action is None:
            action = self.env.action_space(self.name).sample()
        else:
            # Choose an action that beats the opponent's previous action
            if self.opponent_previous_action == 0:
                action = 1  # Play paper to beat rock
            elif self.opponent_previous_action == 1:
                action = 2  # Play scissors to beat paper
            else:
                action = 0  # Play rock to beat scissors
        
        self.history["actions"].append(action)
        return action
```

In this updated version, the agent keeps track of the opponent's previous action and chooses an action that beats it. If the opponent's previous action is unknown (i.e., None), the agent chooses a random action.
====================

Prompt: 
You are given a task to write a game agent to interact with a pettingzoo environment.

The environment is defined as follows:
# Rock Paper Scissors
| Import             | `from pettingzoo.classic import rps_v2` |
|--------------------|-----------------------------------------|
| Actions            | Discrete                                |
| Parallel API       | Yes                                     |
| Manual Control     | No                                      |
| Agents             | `agents= ['player_0', 'player_1']`      |
| Agents             | 2                                       |
| Action Shape       | Discrete(3)                             |
| Action Values      | Discrete(3)                             |
| Observation Shape  | Discrete(4)                             |
| Observation Values | Discrete(4)                             |


Rock, Paper, Scissors is a 2-player hand game where each player chooses either rock, paper or scissors and reveals their choices simultaneously. If both players make the same choice, then it is a draw. However, if their choices are different, the winner is determined as follows: rock beats
scissors, scissors beat paper, and paper beats rock.

### Arguments

``` python
rps_v2.env(max_cycles=15)
```
`max_cycles`:  after max_cycles steps all agents will return done.

### Observation Space

#### Rock, Paper, Scissors

If 3 actions are required, the game played is the standard Rock, Paper, Scissors. The observation is the last opponent action and its space is a scalar value with 4 possible values. Since both players reveal their choices at the same time, the observation is None until both players have acted.
Therefore, 3 represents no action taken yet. Rock is represented with 0, paper with 1 and scissors with 2.

| Value  |  Observation |
| :----: | :---------:  |
| 0      | Rock         |
| 1      | Paper        |
| 2      | Scissors     |
| 3      | None         |


### Action Space

#### Rock, Paper, Scissors

The action space is a scalar value with 3 possible values. The values are encoded as follows: Rock is 0, paper is 1 and scissors is 2.

| Value  |  Action |
| :----: | :---------:  |
| 0      | Rock         |
| 1      | Paper        |
| 2      | Scissors     |

### Rewards

| Winner | Loser |
| :----: | :---: |
| +1     | -1    |

If the game ends in a draw, both players will receive a reward of 0.

The code describe how your agent will interact with the environment:
```python
from pettingzoo.classic import game_name_v0

# create the environment
env = game_name_v0.env()

# create your agent and the opponent agent
agents = {
    env.possible_agents[0]: Agent(env, env.possible_agents[0]),
    env.possible_agents[1]: OpponentAgent(env, env.possible_agents[1])
}

def simulate(agents: any, env: AECEnv) -> dict[any, float]:
    env.reset()
    rewards = {agent: 0 for agent in env.possible_agents}
    
    for agent in agents.values():
        agent.reset()

    for agent_name in env.agent_iter():
        observation, reward, termination, truncation, info = env.last()
        rewards[agent_name] += reward
        obs_message = agents[agent_name].observe(
            observation, reward, termination, truncation, info
        )
        if termination or truncation:
            action = None
        else:
            action = agents[agent_name].act()
        print(f"Agen {agent_name} Action {action}")
        env.step(action)
    env.close()
    return rewards

simulate(agents, env)
```

Here is a starter code to help you get started with coding the agent class,
you can replace the starter code with your own code and add more methods if needed,
but you should not change the method signatures:
# This is the starting point of your agent class, you should not change the existing method signatures.
# However, you can add more methods or change the implementations to obtain a better strategy.
class Agent:
    # import the necessary packages here
    import random
    # initialize the agent's state here
    def __init__(self, env, name):
        self.env = env
        self.name = name
        self.history = {
            "observations": [],
            "actions": []
        }

    # reset the agent's state here
    def reset(self):
        """
        Reset the agent's state.
        """
        self.history = {
            "observations": [],
            "actions": []
        }
    
    # update the agent's state here
    # This is an example of storing the observation.
    def observe(self, observation, reward, termination, truncation, info):
        self.history["observations"].append(observation)
    
    # return the action here
    # write comments to explain why this is a good policy.
    # this is an example of choosing the action randomly, you may want to change it.
    # self.env.action_space is a method, not a subscriptable object.
    def act(self):
        action = self.env.action_space(self.name).sample()
        self.history["actions"].append(action)
        return action

Now, solve the task with the following steps, "the only code block you need to write is the new agent class":
- Think step by step, recap some basic game theory knowledge and apply them to the problem.
- Given the starter code, you should write a new Agent class to interact with the environment and maximize your reward. 
For the code part, you should:
- Enclose the code by ```python and ```
- Only write the Agent class
- `import` need to be under the Agent scope.
- Write comments to explain your code
Response: 
```python
class Agent:
    import random
    
    def __init__(self, env, name):
        self.env = env
        self.name = name
        self.history = {
            "observations": [],
            "actions": []
        }
    
    def reset(self):
        self.history = {
            "observations": [],
            "actions": []
        }
    
    def observe(self, observation, reward, termination, truncation, info):
        self.history["observations"].append(observation)
    
    def act(self):
        # Get the last opponent action
        opponent_action = self.history["observations"][-1]
        
        # Choose a random action
        action = self.random.choice([0, 1, 2])
        
        # Store the chosen action
        self.history["actions"].append(action)
        
        return action
```

In this agent class, the `act` method chooses a random action from the available actions (rock, paper, scissors). The chosen action is then stored in the agent's history.
====================

