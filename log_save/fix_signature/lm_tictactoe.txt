Prompt: 
You are given a task to write a game agent to interact with a pettingzoo environment.

Here is the some information regarding the environment:
| Import             | `from pettingzoo.classic import tictactoe_v3` |
|--------------------|-----------------------------------------------|
| Actions            | Discrete                                      |
| Parallel API       | Yes                                           |
| Manual Control     | No                                            |
| Agents             | `agents= ['player_1', 'player_2']`            |
| Agents             | 2                                             |
| Action Shape       | (1)                                           |
| Action Values      | [0, 8]                                        |
| Observation Shape  | (3, 3, 2)                                     |
| Observation Values | [0,1]                                         |
 
 
Tic-tac-toe is a simple turn based strategy game where 2 players, X and O, take turns marking spaces on a 3 x 3 grid. The first player to place 3 of their marks in a horizontal, vertical, or diagonal line is the winner.
 
### Observation Space
The observation is a dictionary which contains an `'observation'` element which is the usual RL observation described below, and an  `'action_mask'` which holds the legal moves, described in the Legal Actions Mask section.
 
The main observation is 2 planes of the 3x3 board. For player_1, the first plane represents the placement of Xs, and the second plane shows the placement of Os. The possible values for each cell are 0 or 1; in the first plane, 1 indicates that an X has been placed in that cell, and 0 indicates
that X is not in that cell. Similarly, in the second plane, 1 indicates that an O has been placed in that cell, while 0 indicates that an O has not been placed. For player_2, the observation is the same, but Xs and Os swap positions, so Os are encoded in plane 1 and Xs in plane 2. This allows for
self-play.
 
#### Legal Actions Mask
 
The legal moves available to the current agent are found in the `action_mask` element of the dictionary observation. The `action_mask` is a binary vector where each index of the vector represents whether the action is legal or not. The `action_mask` will be all zeros for any agent except the one
whose turn it is. Taking an illegal move ends the game with a reward of -1 for the illegally moving agent and a reward of 0 for all other agents.
 
### Action Space
 
Each action from 0 to 8 represents placing either an X or O in the corresponding cell. The cells are indexed as follows:
 
 
```
0 | 3 | 6
_________
 
1 | 4 | 7
_________
 
2 | 5 | 8
```
 
### Rewards
 
| Winner | Loser |
| :----: | :---: |
| +1     | -1    |
 
If the game ends in a draw, both players will receive a reward of 0.

The code describe how your agent will interact with the environment:
```python
from pettingzoo.classic import game_name_v0

# create the environment
env = game_name_v0.env()

# create your agent and the opponent agent
agents = {
    env.possible_agents[0]: Agent(env, env.possible_agents[0]),
    env.possible_agents[1]: OpponentAgent(env, env.possible_agents[1])
}

def simulate(agents: any, env: AECEnv) -> dict[any, float]:
    env.reset()
    rewards = {agent: 0 for agent in env.possible_agents}
    
    for agent in agents.values():
        agent.reset()

    for agent_name in env.agent_iter():
        observation, reward, termination, truncation, info = env.last()
        rewards[agent_name] += reward
        obs_message = agents[agent_name].observe(
            observation, reward, termination, truncation, info
        )
        if termination or truncation:
            action = None
        else:
            action = agents[agent_name].act()
        print(f"Agen {agent_name} Action {action}")
        env.step(action)
    env.close()
    return rewards

simulate(agents, env)
```

Here is a starter code to help you get started with coding the agent class,
you can replace the starter code with your own code and add more methods if needed,
but you should not change the method signatures:
# This is the starting point of your agent class, you should not change the existing method signatures.
# However, you can add more methods or change the implementations to obtain a better strategy.
class Agent:
    # You can initialize the agent's state here
    def __init__(self, env, name):
        self.env = env
        self.name = name
        self.history = {
            "observations": [],
            "actions": []
        }

    # You can reset the agent's state here
    def reset(self):
        """
        Reset the agent's state.
        """
        self.history = {
            "observations": [],
            "actions": []
        }
    
    # You can update the agent's state
    # This is an example of storing the observation.
    def observe(self, observation, reward, termination, truncation, info):
        self.history["observations"].append(observation)
    
    # You should return the action you want to take and write a docstring to explain why it is a good action.
    # This is an example of choosing the action randomly.
    def act(self):
        action = self.env.action_space(self.name).sample()  # Becareful self.env.action_space() not a list, don't change it to self.env.action_space[] 
        self.history["actions"].append(action)
        return action

Now, write the Agent class to interact with the environment and maximize your reward. 
For each method, you should write comments to describe the method.
For the act() method, you should return the action you want to take and write comments to explain why it is a good action.
Your code should be enclosed by ```python and ```.
Return the Agent class only.
Response: 
```python
import numpy as np

class Agent:
    def __init__(self, env, name):
        self.env = env
        self.name = name
        self.history = {
            "observations": [],
            "actions": []
        }

    def reset(self):
        self.history = {
            "observations": [],
            "actions": []
        }
    
    def observe(self, observation, reward, termination, truncation, info):
        self.history["observations"].append(observation)
    
    def act(self):
        observation = self.history["observations"][-1]
        action_mask = observation["action_mask"]
        legal_actions = np.where(action_mask == 1)[0]
        action = np.random.choice(legal_actions)
        self.history["actions"].append(action)
        return action
```

In the `act()` method, we first retrieve the latest observation from the agent's history. We then extract the action mask from the observation, which represents the legal moves available to the agent. We filter out the indices where the action mask is 1, indicating a legal move. We then randomly choose one of the legal actions and return it as the agent's action.
====================
