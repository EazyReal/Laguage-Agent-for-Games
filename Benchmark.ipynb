{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pettingzoo.classic import tictactoe_v3\n",
    "# import numpy as np\n",
    "\n",
    "# class TicTacToeAgent:\n",
    "#     def __init__(self):\n",
    "#         self.name = 'TicTacToeAgent'\n",
    "#         self.env = tictactoe_v3.env()\n",
    "#         self.env.reset()\n",
    "#         self.action_space = [0,1,2,3,4,5,6,7,8]\n",
    "\n",
    "#     def reset(self):\n",
    "#         self.env.reset()\n",
    "\n",
    "#     def act(self, observation, player):\n",
    "#         board, action_mask = observation['observation'], observation['action_mask']\n",
    "#         legal_moves = [i for i in range(9) if action_mask[i] == 1]\n",
    "        \n",
    "#         # Check for win or block\n",
    "#         for move in legal_moves:\n",
    "#             if self.is_winning_move(board, move, player):\n",
    "#                 return move\n",
    "#             if self.is_winning_move(board, move, 1 - player):\n",
    "#                 return move\n",
    "\n",
    "#         # If no win or block move, pick a random legal move\n",
    "#         return np.random.choice(legal_moves)\n",
    "\n",
    "#     def is_winning_move(self, board, move, player):\n",
    "#         temp_board = board.copy()\n",
    "#         temp_board[move // 3, move % 3, player] = 1\n",
    "#         return self.is_winner(temp_board, player)\n",
    "\n",
    "#     def is_winner(self, board, player):\n",
    "#         # Check rows, columns and diagonals for a win\n",
    "#         for i in range(3):\n",
    "#             if all(board[i,:,player] == 1) or all(board[:,i,player] == 1):\n",
    "#                 return True\n",
    "#         if board[0,0,player] == board[1,1,player] == board[2,2,player] == 1 or board[0,2,player] == board[1,1,player] == board[2,0,player] == 1:\n",
    "#             return True\n",
    "#         return False\n",
    "\n",
    "# # Example usage\n",
    "# agent = TicTacToeAgent()\n",
    "# # This is how you would typically use it in a PettingZoo loop\n",
    "# for agent_id in agent.env.agent_iter():\n",
    "#     observation, _, _, _, _ = agent.env.last()\n",
    "#     if agent_id == agent.name:\n",
    "#         action = agent.act(observation, agent.env.agent_selection)\n",
    "#         agent.env.step(action)\n",
    "#     else:\n",
    "#         agent.env.step(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "# class Agent:\n",
    "#     def __init__(self):\n",
    "#         self.env = tictactoe_v3.env()\n",
    "#         self.env.reset()\n",
    "#         self.action_space = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "#     def reset(self):\n",
    "#         self.env.reset()\n",
    "        \n",
    "#     def observe(self, observation, reward, termination, truncation, info):\n",
    "#         self.current_observation = observation\n",
    "#         obs_message = f\"Observation: {observation}\"\n",
    "#         return obs_message\n",
    "\n",
    "#     def act(self, observation, player):\n",
    "#         board, action_mask = observation['observation'], observation['action_mask']\n",
    "#         valid_actions = np.where(action_mask == 1)[0]\n",
    "\n",
    "#         # Check for potential wins or blocks\n",
    "#         for action in valid_actions:\n",
    "#             simulated_board = self.simulate_action(board, action, player)\n",
    "#             if self.is_winner(simulated_board, player):\n",
    "#                 return action\n",
    "\n",
    "#         # If no immediate win or block, choose a random valid action\n",
    "#         return np.random.choice(valid_actions)\n",
    "\n",
    "#     def simulate_action(self, board, action, player):\n",
    "#         simulated_board = np.copy(board)\n",
    "#         row, col = divmod(action, 3)\n",
    "#         simulated_board[row, col, player] = 1\n",
    "#         return simulated_board\n",
    "\n",
    "#     def is_winner(self, board, player):\n",
    "#         # Check rows, columns, and diagonals for a win\n",
    "#         for i in range(3):\n",
    "#             if np.all(board[i, :, player] == 1) or np.all(board[:, i, player] == 1):\n",
    "#                 return True\n",
    "#         if board[0, 0, player] == board[1, 1, player] == board[2, 2, player] == 1:\n",
    "#             return True\n",
    "#         if board[0, 2, player] == board[1, 1, player] == board[2, 0, player] == 1:\n",
    "#             return True\n",
    "#         return False\n",
    "\n",
    "# # Example of how to use the Agent\n",
    "# def simulate(agents, env):\n",
    "#     env.reset()\n",
    "#     Total_reward = 0\n",
    "    \n",
    "#     for agent_name in env.agent_iter():\n",
    "#         observation, reward, termination, truncation, info = env.last()\n",
    "#         Total_reward += reward\n",
    "        \n",
    "#         obs_message = agents[agent_name].observe(\n",
    "#             observation, reward, termination, truncation, info\n",
    "#         )\n",
    "#         # print(obs_message)\n",
    "#         if termination or truncation:\n",
    "#             action = None\n",
    "#         else:\n",
    "#             action = agents[agent_name].act(observation)\n",
    "#         print(f\"Action: {action}\")\n",
    "#         env.step(action)\n",
    "#     env.close()\n",
    "#     print(\"Total reward: \", Total_reward)\n",
    "\n",
    "# # Initialize environment and agents\n",
    "# env = tictactoe_v3.env()\n",
    "# agents = {'player_1': Agent(), 'player_2': Agent()}\n",
    "\n",
    "# # Simulate the game\n",
    "# simulate(agents, env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blocking Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 6\n",
      "Action: 5\n",
      "Action: 4\n",
      "Action: 1\n",
      "Action: 7\n",
      "Action: 3\n",
      "Action: 8\n",
      "Action: None\n",
      "Action: None\n",
      "Total reward:  0\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.classic import tictactoe_v3\n",
    "import numpy as np\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.observation_space = None\n",
    "        self.action_space = None\n",
    "        self.current_observation = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_observation = None\n",
    "\n",
    "    def observe(self, observation, reward, termination, truncation, info):\n",
    "        self.current_observation = observation\n",
    "        obs_message = f\"Observation: {observation}\"\n",
    "        return obs_message\n",
    "\n",
    "    def act(self):\n",
    "        # Simple strategy: first try to win, then block, else random\n",
    "        action = self.find_winning_move()\n",
    "        if action is None:\n",
    "            action = self.find_blocking_move()\n",
    "        if action is None:\n",
    "            action = self.random_move()\n",
    "        return action\n",
    "\n",
    "    def find_winning_move(self):\n",
    "        return self.find_best_move(is_winning_move=True)\n",
    "\n",
    "    def find_blocking_move(self):\n",
    "        return self.find_best_move(is_winning_move=False)\n",
    "\n",
    "    def find_best_move(self, is_winning_move):\n",
    "        board, action_mask = self.current_observation['observation'], self.current_observation['action_mask']\n",
    "        for action in range(9):\n",
    "            if action_mask[action]:\n",
    "                simulated_board = self.simulate_move(board, action, is_winning_move)\n",
    "                if self.is_winner(simulated_board, is_winning_move):\n",
    "                    return action\n",
    "        return None\n",
    "\n",
    "    def random_move(self):\n",
    "        action_mask = self.current_observation['action_mask']\n",
    "        legal_actions = [action for action in range(9) if action_mask[action]]\n",
    "        return np.random.choice(legal_actions) if legal_actions else None\n",
    "\n",
    "    def simulate_move(self, board, action, is_winning_move):\n",
    "        simulated_board = np.copy(board)\n",
    "        player_index = 0 if is_winning_move else 1\n",
    "        row, col = action % 3, action // 3\n",
    "        simulated_board[row, col, player_index] = 1\n",
    "        return simulated_board\n",
    "\n",
    "    def is_winner(self, board, is_winning_move):\n",
    "        player_index = 0 if is_winning_move else 1\n",
    "        # Check rows, columns, and diagonals\n",
    "        for i in range(3):\n",
    "            if np.all(board[:, i, player_index] == 1) or np.all(board[i, :, player_index] == 1):\n",
    "                return True\n",
    "        if np.all([board[i, i, player_index] == 1 for i in range(3)]) or np.all([board[i, 2-i, player_index] == 1 for i in range(3)]):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "def simulate(agents, env):\n",
    "    env.reset()\n",
    "    Total_reward = 0\n",
    "    \n",
    "    for agent_name in env.agent_iter():\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "        Total_reward += reward\n",
    "        \n",
    "        obs_message = agents[agent_name].observe(\n",
    "            observation, reward, termination, truncation, info\n",
    "        )\n",
    "        # print(obs_message)\n",
    "        if termination or truncation:\n",
    "            action = None\n",
    "        else:\n",
    "            action = agents[agent_name].act()\n",
    "        print(f\"Action: {action}\")\n",
    "        env.step(action)\n",
    "    env.close()\n",
    "    print(\"Total reward: \", Total_reward)\n",
    "\n",
    "# Initialize environment and agents\n",
    "env = tictactoe_v3.env()\n",
    "agents = {'player_1': Agent(), 'player_2': Agent()}\n",
    "\n",
    "# Simulate the game\n",
    "simulate(agents, env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: {'observation': array([[[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]],\n",
      "\n",
      "       [[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]],\n",
      "\n",
      "       [[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]]], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int8)}, Reward: 0\n",
      "Action: 0\n",
      "Observation: {'observation': array([[[0, 1],\n",
      "        [0, 0],\n",
      "        [0, 0]],\n",
      "\n",
      "       [[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]],\n",
      "\n",
      "       [[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]]], dtype=int8), 'action_mask': array([0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int8)}, Reward: 0\n",
      "Action: 0\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "Observation: {'observation': array([[[1, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]],\n",
      "\n",
      "       [[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]],\n",
      "\n",
      "       [[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]]], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int8)}, Reward: 0\n",
      "Action: None\n",
      "Observation: {'observation': array([[[0, 1],\n",
      "        [0, 0],\n",
      "        [0, 0]],\n",
      "\n",
      "       [[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]],\n",
      "\n",
      "       [[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]]], dtype=int8), 'action_mask': array([0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int8)}, Reward: -1.0\n",
      "Action: None\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.classic import tictactoe_v3\n",
    "import numpy as np\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, mark):\n",
    "        self.mark = mark\n",
    "        self.opponent_mark = 'X' if mark == 'O' else 'O'\n",
    "        self.action_space = list(range(9))\n",
    "        self.board = np.zeros((3, 3), dtype=str)\n",
    "\n",
    "    def act(self):\n",
    "        best_score = -float('inf')\n",
    "        best_action = None\n",
    "\n",
    "        legal_actions = [action for action in self.action_space if self.is_legal_action(action)]\n",
    "\n",
    "        for action in legal_actions:\n",
    "            self.make_move(action, self.mark)\n",
    "            score = self.minimax(0, False)\n",
    "            self.undo_move(action)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_action = action\n",
    "\n",
    "        return best_action if best_action is not None else np.random.choice(legal_actions)\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((3, 3), dtype=str)\n",
    "\n",
    "    def observe(self, observation, reward, termination, truncation, info):\n",
    "        self.update_board(observation)\n",
    "        return f\"Observation: {observation}, Reward: {reward}\"\n",
    "\n",
    "    def minimax(self, depth, is_maximizing):\n",
    "        if self.check_winner(self.mark):\n",
    "            return 1\n",
    "        elif self.check_winner(self.opponent_mark):\n",
    "            return -1\n",
    "        elif np.all(self.board != ''):\n",
    "            return 0\n",
    "\n",
    "        if is_maximizing:\n",
    "            best_score = -float('inf')\n",
    "            for action in self.action_space:\n",
    "                if self.is_legal_action(action):\n",
    "                    self.make_move(action, self.mark)\n",
    "                    score = self.minimax(depth + 1, False)\n",
    "                    self.undo_move(action)\n",
    "                    best_score = max(best_score, score)\n",
    "            return best_score\n",
    "        else:\n",
    "            best_score = float('inf')\n",
    "            for action in self.action_space:\n",
    "                if self.is_legal_action(action):\n",
    "                    self.make_move(action, self.opponent_mark)\n",
    "                    score = self.minimax(depth + 1, True)\n",
    "                    self.undo_move(action)\n",
    "                    best_score = min(best_score, score)\n",
    "            return best_score\n",
    "\n",
    "    def is_legal_action(self, action):\n",
    "        x, y = divmod(action, 3)\n",
    "        return self.board[x, y] == ''\n",
    "\n",
    "    def make_move(self, action, mark):\n",
    "        x, y = divmod(action, 3)\n",
    "        self.board[x, y] = mark\n",
    "\n",
    "    def undo_move(self, action):\n",
    "        x, y = divmod(action, 3)\n",
    "        self.board[x, y] = ''\n",
    "\n",
    "    def update_board(self, observation):\n",
    "        player_plane, opponent_plane = np.array(observation['observation']).reshape((2, 3, 3))\n",
    "        self.board = np.where(player_plane == 1, self.mark, '')\n",
    "        self.board = np.where(opponent_plane == 1, self.opponent_mark, self.board)\n",
    "\n",
    "\n",
    "    def check_winner(self, mark):\n",
    "        for row in self.board:\n",
    "            if np.all(row == mark):\n",
    "                return True\n",
    "        for col in self.board.T:\n",
    "            if np.all(col == mark):\n",
    "                return True\n",
    "        if np.all(np.diag(self.board) == mark) or np.all(np.diag(np.fliplr(self.board)) == mark):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "# Instantiate the environment and agents\n",
    "env = tictactoe_v3.env()\n",
    "agents = {'player_1': Agent('X'), 'player_2': Agent('O')}\n",
    "\n",
    "def simulate(agents, env):\n",
    "    env.reset()\n",
    "    \n",
    "    for agent_name in env.agent_iter():\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "        obs_message = agents[agent_name].observe(\n",
    "            observation, reward, termination, truncation, info\n",
    "        )\n",
    "        print(obs_message)\n",
    "        if termination or truncation:\n",
    "            action = None\n",
    "        else:\n",
    "            action = agents[agent_name].act()\n",
    "        print(f\"Action: {action}\")\n",
    "        env.step(action)\n",
    "    env.close()\n",
    "\n",
    "simulate(agents, env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only act on the next available action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 0\n",
      "Action: 1\n",
      "Action: 2\n",
      "Action: 3\n",
      "Action: 4\n",
      "Action: 5\n",
      "Action: 6\n",
      "Action: None\n",
      "Total reward:  -1\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.classic import tictactoe_v3\n",
    "import numpy as np\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.action_space = list(range(9))  # Possible actions from 0 to 8\n",
    "        self.current_observation = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_observation = None\n",
    "\n",
    "    def observe(self, observation, reward, termination, truncation, info):\n",
    "        self.current_observation = observation\n",
    "        return f\"Observation: {observation}, Reward: {reward}\"\n",
    "\n",
    "    def act(self):\n",
    "        # Implement a simple strategy: Choose the first available legal move\n",
    "        action_mask = self.current_observation['action_mask']\n",
    "        for action, legal in enumerate(action_mask):\n",
    "            if legal:\n",
    "                return action\n",
    "        return None  # In case no legal moves are available\n",
    "\n",
    "def simulate(agents, env):\n",
    "    env.reset()\n",
    "    for agent in agents.values():\n",
    "        agent.reset()\n",
    "\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        agent_name = env.agent_selection\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "        obs_message = agents[agent_name].observe(\n",
    "            observation, reward, termination, truncation, info\n",
    "        )\n",
    "        # print(obs_message)\n",
    "        total_reward += reward\n",
    "        if termination or truncation:\n",
    "            action = None\n",
    "        else:\n",
    "            action = agents[agent_name].act()\n",
    "        print(f\"Action: {action}\")\n",
    "        env.step(action)\n",
    "        if termination or truncation:\n",
    "            break\n",
    "    env.close()\n",
    "    print(\"Total reward: \", total_reward)\n",
    "\n",
    "# Initialize the environment and agents\n",
    "env = tictactoe_v3.env()\n",
    "agents = {'player_1': Agent(), 'player_2': Agent()}\n",
    "\n",
    "# Simulate a game\n",
    "simulate(agents, env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: {'observation': array([[[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]],\n",
      "\n",
      "       [[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]],\n",
      "\n",
      "       [[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]]], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int8)}, Reward: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/sabrinachiang/Desktop/2023 Fall/DL Text Data 8803/Project/Code/Game_Langchain/Benchmark.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m agents \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mplayer_1\u001b[39m\u001b[39m'\u001b[39m: Agent(), \u001b[39m'\u001b[39m\u001b[39mplayer_2\u001b[39m\u001b[39m'\u001b[39m: Agent()}\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=133'>134</a>\u001b[0m \u001b[39m# Simulate the game\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=134'>135</a>\u001b[0m simulate(agents, env)\n",
      "\u001b[1;32m/Users/sabrinachiang/Desktop/2023 Fall/DL Text Data 8803/Project/Code/Game_Langchain/Benchmark.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m     action \u001b[39m=\u001b[39m agents[agent_name]\u001b[39m.\u001b[39;49mact()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=124'>125</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAction: \u001b[39m\u001b[39m{\u001b[39;00maction\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=125'>126</a>\u001b[0m env\u001b[39m.\u001b[39mstep(action)\n",
      "\u001b[1;32m/Users/sabrinachiang/Desktop/2023 Fall/DL Text Data 8803/Project/Code/Game_Langchain/Benchmark.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mif\u001b[39;00m action_mask[action] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     board[action \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m3\u001b[39m, action \u001b[39m%\u001b[39m \u001b[39m3\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplayer_id\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     move_val \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mminimax(board, \u001b[39m0\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     board[action \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m3\u001b[39m, action \u001b[39m%\u001b[39m \u001b[39m3\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m  \u001b[39m# Undo the move\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39mif\u001b[39;00m move_val \u001b[39m>\u001b[39m best_val:\n",
      "\u001b[1;32m/Users/sabrinachiang/Desktop/2023 Fall/DL Text Data 8803/Project/Code/Game_Langchain/Benchmark.ipynb Cell 9\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m         \u001b[39mif\u001b[39;00m board[i][j] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m             board[i][j] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplayer_id\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m             best \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(best, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mminimax(board, depth \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m, \u001b[39mnot\u001b[39;49;00m is_max))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m             board[i][j] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39mreturn\u001b[39;00m best\n",
      "\u001b[1;32m/Users/sabrinachiang/Desktop/2023 Fall/DL Text Data 8803/Project/Code/Game_Langchain/Benchmark.ipynb Cell 9\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m         \u001b[39mif\u001b[39;00m board[i][j] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m             board[i][j] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplayer_id\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m             best \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(best, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mminimax(board, depth \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m, \u001b[39mnot\u001b[39;49;00m is_max))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m             board[i][j] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39mreturn\u001b[39;00m best\n",
      "\u001b[1;32m/Users/sabrinachiang/Desktop/2023 Fall/DL Text Data 8803/Project/Code/Game_Langchain/Benchmark.ipynb Cell 9\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m         \u001b[39mif\u001b[39;00m board[i][j] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m             board[i][j] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplayer_id\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m             best \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(best, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mminimax(board, depth \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m, \u001b[39mnot\u001b[39;49;00m is_max))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m             board[i][j] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39mreturn\u001b[39;00m best\n",
      "\u001b[1;32m/Users/sabrinachiang/Desktop/2023 Fall/DL Text Data 8803/Project/Code/Game_Langchain/Benchmark.ipynb Cell 9\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m         \u001b[39mif\u001b[39;00m board[i][j] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m             board[i][j] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplayer_id\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m             best \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(best, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mminimax(board, depth \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m, \u001b[39mnot\u001b[39;49;00m is_max))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m             board[i][j] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39mreturn\u001b[39;00m best\n",
      "    \u001b[0;31m[... skipping similar frames: Agent.minimax at line 67 (5 times), Agent.minimax at line 58 (4 times)]\u001b[0m\n",
      "\u001b[1;32m/Users/sabrinachiang/Desktop/2023 Fall/DL Text Data 8803/Project/Code/Game_Langchain/Benchmark.ipynb Cell 9\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m         \u001b[39mif\u001b[39;00m board[i][j] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m             board[i][j] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplayer_id\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m             best \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(best, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mminimax(board, depth \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m, \u001b[39mnot\u001b[39;49;00m is_max))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m             board[i][j] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39mreturn\u001b[39;00m best\n",
      "\u001b[1;32m/Users/sabrinachiang/Desktop/2023 Fall/DL Text Data 8803/Project/Code/Game_Langchain/Benchmark.ipynb Cell 9\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m         \u001b[39mif\u001b[39;00m board[i][j] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m             board[i][j] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplayer_id\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m             best \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(best, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mminimax(board, depth \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m, \u001b[39mnot\u001b[39;49;00m is_max))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m             board[i][j] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39mreturn\u001b[39;00m best\n",
      "\u001b[1;32m/Users/sabrinachiang/Desktop/2023 Fall/DL Text Data 8803/Project/Code/Game_Langchain/Benchmark.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mif\u001b[39;00m score \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m10\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m score\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mis_moves_left(board):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mif\u001b[39;00m is_max:\n",
      "\u001b[1;32m/Users/sabrinachiang/Desktop/2023 Fall/DL Text Data 8803/Project/Code/Game_Langchain/Benchmark.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_moves_left\u001b[39m(\u001b[39mself\u001b[39m, b):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sabrinachiang/Desktop/2023%20Fall/DL%20Text%20Data%208803/Project/Code/Game_Langchain/Benchmark.ipynb#X12sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49many(b \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniforge3/envs/Game/lib/python3.10/site-packages/numpy/core/fromnumeric.py:2412\u001b[0m, in \u001b[0;36many\u001b[0;34m(a, axis, out, keepdims, where)\u001b[0m\n\u001b[1;32m   2322\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_any_dispatcher)\n\u001b[1;32m   2323\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39many\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue, \u001b[39m*\u001b[39m, where\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue):\n\u001b[1;32m   2324\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2325\u001b[0m \u001b[39m    Test whether any array element along a given axis evaluates to True.\u001b[39;00m\n\u001b[1;32m   2326\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2410\u001b[0m \n\u001b[1;32m   2411\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2412\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapreduction(a, np\u001b[39m.\u001b[39;49mlogical_or, \u001b[39m'\u001b[39;49m\u001b[39many\u001b[39;49m\u001b[39m'\u001b[39;49m, axis, \u001b[39mNone\u001b[39;49;00m, out,\n\u001b[1;32m   2413\u001b[0m                           keepdims\u001b[39m=\u001b[39;49mkeepdims, where\u001b[39m=\u001b[39;49mwhere)\n",
      "File \u001b[0;32m~/miniforge3/envs/Game/lib/python3.10/site-packages/numpy/core/fromnumeric.py:75\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrapreduction\u001b[39m(obj, ufunc, method, axis, dtype, out, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     72\u001b[0m     passkwargs \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems()\n\u001b[1;32m     73\u001b[0m                   \u001b[39mif\u001b[39;00m v \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39m_NoValue}\n\u001b[0;32m---> 75\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(obj) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m mu\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m     76\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m             reduction \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, method)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pettingzoo.classic import tictactoe_v3\n",
    "import numpy as np\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.action_space = list(range(9))  # Possible actions from 0 to 8\n",
    "        self.current_observation = None\n",
    "        self.player_id = 1  # Assuming the agent is player 1 (X)\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_observation = None\n",
    "\n",
    "    def observe(self, observation, reward, termination, truncation, info):\n",
    "        self.current_observation = observation\n",
    "        return f\"Observation: {observation}, Reward: {reward}\"\n",
    "\n",
    "    def act(self):\n",
    "        board, action_mask = self.get_board_and_mask(self.current_observation)\n",
    "        best_val = -np.inf\n",
    "        best_move = None\n",
    "\n",
    "        for action in self.action_space:\n",
    "            if action_mask[action] == 1:\n",
    "                board[action // 3, action % 3] = self.player_id\n",
    "                move_val = self.minimax(board, 0, False)\n",
    "                board[action // 3, action % 3] = 0  # Undo the move\n",
    "\n",
    "                if move_val > best_val:\n",
    "                    best_val = move_val\n",
    "                    best_move = action\n",
    "\n",
    "        return best_move if best_move is not None else 0  # Default to 0 if no valid move found\n",
    "\n",
    "    def evaluate(self, b):\n",
    "        # Implement the evaluation logic here\n",
    "        # Return a score for the board state\n",
    "        pass\n",
    "\n",
    "    def is_moves_left(self, b):\n",
    "        return np.any(b == 0)\n",
    "\n",
    "    def minimax(self, board, depth, is_max):\n",
    "        score = self.evaluate(board)\n",
    "\n",
    "        if score == 10:\n",
    "            return score\n",
    "        if score == -10:\n",
    "            return score\n",
    "        if not self.is_moves_left(board):\n",
    "            return 0\n",
    "\n",
    "        if is_max:\n",
    "            best = -np.inf\n",
    "            for i in range(3):\n",
    "                for j in range(3):\n",
    "                    if board[i][j] == 0:\n",
    "                        board[i][j] = self.player_id\n",
    "                        best = max(best, self.minimax(board, depth + 1, not is_max))\n",
    "                        board[i][j] = 0\n",
    "            return best\n",
    "        else:\n",
    "            best = np.inf\n",
    "            for i in range(3):\n",
    "                for j in range(3):\n",
    "                    if board[i][j] == 0:\n",
    "                        board[i][j] = 1 - self.player_id\n",
    "                        best = min(best, self.minimax(board, depth + 1, not is_max))\n",
    "                        board[i][j] = 0\n",
    "            return best\n",
    "\n",
    "    def get_board_and_mask(self, observation):\n",
    "        board = np.zeros((3, 3), dtype=int)\n",
    "        action_mask = observation['action_mask']\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                if observation['observation'][i][j][0] == 1:\n",
    "                    board[i, j] = 1  # Player 1's mark\n",
    "                elif observation['observation'][i][j][1] == 1:\n",
    "                    board[i, j] = -1  # Player 2's mark\n",
    "        return board, action_mask\n",
    "\n",
    "# def simulate(agents, env):\n",
    "#     env.reset()\n",
    "#     for agent in agents.values():\n",
    "#         agent.reset()\n",
    "\n",
    "#     while True:\n",
    "#         agent_name = env.agent_selection\n",
    "#         observation, reward, termination, truncation, info = env.last()\n",
    "#         obs_message = agents[agent_name].observe(observation, reward, termination, truncation, info)\n",
    "#         print(obs_message)\n",
    "#         if termination or truncation:\n",
    "#             action = None\n",
    "#         else:\n",
    "#             action = agents[agent_name].act()\n",
    "#         print(f\"Action: {action}\")\n",
    "#         env.step(action)\n",
    "#         if termination or truncation:\n",
    "#             break\n",
    "#     env.close()\n",
    "\n",
    "# # Initialize the environment and agents\n",
    "# env = tictactoe_v3.env()\n",
    "# agents = {'player_1': Agent(), 'player_2': Agent()}\n",
    "\n",
    "# # Simulate a game\n",
    "# simulate(agents, env)\n",
    "\n",
    "def simulate(agents, env):\n",
    "    env.reset()\n",
    "    Total_reward = 0\n",
    "    \n",
    "    for agent_name in env.agent_iter():\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "        Total_reward += reward\n",
    "        \n",
    "        obs_message = agents[agent_name].observe(\n",
    "            observation, reward, termination, truncation, info\n",
    "        )\n",
    "        print(obs_message)\n",
    "        if termination or truncation:\n",
    "            action = None\n",
    "        else:\n",
    "            action = agents[agent_name].act()\n",
    "        print(f\"Action: {action}\")\n",
    "        env.step(action)\n",
    "    env.close()\n",
    "    print(\"Total reward: \", Total_reward)\n",
    "\n",
    "# Initialize environment and agents\n",
    "env = tictactoe_v3.env()\n",
    "agents = {'player_1': Agent(), 'player_2': Agent()}\n",
    "\n",
    "# Simulate the game\n",
    "simulate(agents, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Game",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
